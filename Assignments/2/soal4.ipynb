{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù…\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "iris = load_iris()\n",
    "X = iris.data  # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ (4 ÙˆÛŒÚ˜Ú¯ÛŒ)\n",
    "y = iris.target  # Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ (3 Ú©Ù„Ø§Ø³)\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "print(f\"Feature names: {iris.feature_names}\")\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ\n",
    "print(\"\\nğŸ“Œ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:\")\n",
    "print(iris.DESCR[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ù†Ø³Ø¨Øª 80% Ø¢Ù…ÙˆØ²Ø´ Ùˆ 20% ØªØ³Øª\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140efc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø´Ø¯Ù†Ø¯.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§ØªØ´ÙˆÙ†\n",
    "models = []\n",
    "\n",
    "# 1. Perceptron Ø¨Ø§ max_iter Ù…Ø®ØªÙ„Ù\n",
    "max_iters = [10, 50, 100, 500, 1000]\n",
    "for max_iter in max_iters:\n",
    "    models.append((f'Perceptron (max_iter={max_iter})', Perceptron(max_iter=max_iter, random_state=42)))\n",
    "\n",
    "# 2. Perceptron Ø¨Ø§ eta0 Ù…Ø®ØªÙ„Ù\n",
    "etas = [1, 0.1, 0.01, 0.001]\n",
    "for eta in etas:\n",
    "    models.append((f'Perceptron (eta0={eta})', Perceptron(eta0=eta, random_state=42)))\n",
    "\n",
    "# 3. Perceptron Ø¨Ø§ PCA (Ú©Ø§Ù‡Ø´ Ø¨Ø¹Ø¯ Ø¨Ù‡ 2 Ø¨Ø¹Ø¯)\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "models.append(('Perceptron with PCA (2D)', Perceptron(random_state=42)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù„ÛŒØ³ØªÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬\n",
    "results = {\n",
    "    'Model Name': [],\n",
    "    'Accuracy': [],\n",
    "    'Confusion Matrix': [],\n",
    "    'Converged?': []\n",
    "}\n",
    "\n",
    "# Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "for name, model in models:\n",
    "    print(f\"\\n--- Training: {name} ---\")\n",
    "    \n",
    "    # Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "    if \"PCA\" in name:\n",
    "        X_train_use = X_train_pca\n",
    "        X_test_use = X_test_pca\n",
    "    else:\n",
    "        X_train_use = X_train_scaled\n",
    "        X_test_use = X_test_scaled\n",
    "    \n",
    "    # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
    "    model.fit(X_train_use, y_train)\n",
    "    \n",
    "    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ\n",
    "    y_pred = model.predict(X_test_use)\n",
    "    \n",
    "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    converged = model.n_iter_ < model.max_iter  # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ\n",
    "    \n",
    "    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬\n",
    "    results['Model Name'].append(name)\n",
    "    results['Accuracy'].append(acc)\n",
    "    results['Confusion Matrix'].append(cm)\n",
    "    results['Converged?'].append(converged)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Converged: {converged}\")\n",
    "    print(f\"Iterations: {model.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ØªØ¨Ø¯ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"âœ… Ù†ØªØ§ÛŒØ¬ Ú©Ù„ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§:\")\n",
    "print(\"=\"*120)\n",
    "print(results_df[['Model Name', 'Accuracy', 'Converged?']].round(4).sort_values(by='Accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Accuracy\n",
    "best_idx = results_df['Accuracy'].idxmax()\n",
    "best_model_name = results_df.loc[best_idx, 'Model Name']\n",
    "best_cm = results_df.loc[best_idx, 'Confusion Matrix']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(best_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names)\n",
    "plt.title(f'Confusion Matrix - Best Model:\\n{best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Accuracy\n",
    "best_acc_model = results_df.loc[results_df['Accuracy'].idxmax(), 'Model Name']\n",
    "best_acc = results_df['Accuracy'].max()\n",
    "\n",
    "print(f\"âœ… Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø§Ø² Ù†Ø¸Ø± Accuracy: '{best_acc_model}' Ø¨Ø§ Ø¯Ù‚Øª {best_acc:.4f}\")\n",
    "\n",
    "# Ù…Ù‚Ø§ÛŒØ³Ù‡ max_iter\n",
    "max_iter_models = results_df[results_df['Model Name'].str.contains('max_iter')]\n",
    "print(f\"\\nğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Accuracy Ø¨Ø§ max_iter Ù…Ø®ØªÙ„Ù: {max_iter_models['Accuracy'].mean():.4f}\")\n",
    "\n",
    "# Ù…Ù‚Ø§ÛŒØ³Ù‡ eta0\n",
    "eta0_models = results_df[results_df['Model Name'].str.contains('eta0')]\n",
    "print(f\"ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Accuracy Ø¨Ø§ eta0 Ù…Ø®ØªÙ„Ù: {eta0_models['Accuracy'].mean():.4f}\")\n",
    "\n",
    "# ØªØ£Ø«ÛŒØ± PCA\n",
    "pca_model = results_df[results_df['Model Name'].str.contains('PCA')]\n",
    "print(f\"\\nğŸ“Œ ØªØ£Ø«ÛŒØ± PCA: Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø¨Ø§ PCA {pca_model['Accuracy'].values[0]:.4f} Ø§Ø³Øª. Ø§ÛŒÙ† Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ú©Ø§Ù‡Ø´ Ø¨Ø¹Ø¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ú©Ø§Ù‡Ø´ Ø¯Ù‡Ø¯.\")\n",
    "\n",
    "# Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ\n",
    "print(f\"\\nğŸ” ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù‡Ù…Ú¯Ø±Ø§ Ø´Ø¯Ù†Ø¯: {results_df['Converged?'].sum()} Ø§Ø² {len(results_df)} Ù…Ø¯Ù„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„\n",
    "best_model = models[results_df['Accuracy'].idxmax()][1]\n",
    "if \"PCA\" in best_model_name:\n",
    "    y_pred_best = best_model.predict(X_test_pca)\n",
    "else:\n",
    "    y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"ğŸ“‹ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„:\")\n",
    "print(\"=\"*120)\n",
    "print(classification_report(y_test, y_pred_best, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc99435",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"ğŸ“ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ\")\n",
    "print(\"=\"*120)\n",
    "print(\"\"\"\n",
    "Ø¯Ø± Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ†ØŒ Ù…Ø¯Ù„ Perceptron Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Iris (Ø³Ù‡ Ú©Ù„Ø§Ø³ÛŒ) Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯.\n",
    "\n",
    "Ù†ØªØ§ÛŒØ¬ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡:\n",
    "\n",
    "- **Perceptron** ÛŒÚ© Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø³Ø§Ø¯Ù‡ Ùˆ Ø®Ø·ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø³Ø§Ø¦Ù„ÛŒ Ú©Ù‡ Ù‚Ø§Ø¨Ù„ Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ø®Ø·ÛŒ Ù‡Ø³ØªÙ†Ø¯ØŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø±Ø¯.\n",
    "- **Ù¾Ø§Ø±Ø§Ù…ØªØ± max_iter** ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ù‡ Ú©Ù‡ Ú†Ù†Ø¯ Ø¯ÙˆØ± Ø¢Ù…ÙˆØ²Ø´ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø´Ù‡. Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ø²Ø±Ú¯ØªØ± Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ù†Ø¬Ø± Ø¨Ù‡ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ø¨Ù‡ØªØ± Ø´ÙˆÙ†Ø¯.\n",
    "- **Ù¾Ø§Ø±Ø§Ù…ØªØ± eta0** (Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ) Ù†Ù‚Ø´ Ù…Ù‡Ù…ÛŒ Ø¯Ø± Ø³Ø±Ø¹Øª Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯. Ù…Ù‚Ø§Ø¯ÛŒØ± Ú©ÙˆÚ†Ú©ØªØ± (Ù…Ø«Ù„ 0.01 ÛŒØ§ 0.001) Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ù†Ø¬Ø± Ø¨Ù‡ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ø¨Ù‡ØªØ± Ø´ÙˆÙ†Ø¯.\n",
    "- **PCA** Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø¨Ø¹Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯ØŒ Ø§Ù…Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ø§Ø¹Ø« Ú©Ø§Ù‡Ø´ Ø¯Ù‚Øª Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ú†ÙˆÙ† Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² Ø¯Ø³Øª Ù…ÛŒâ€ŒØ±ÙˆØ¯.\n",
    "- **Perceptron** ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù…Ø³Ø§Ø¦Ù„ÛŒ Ú©Ù‡ Ù‚Ø§Ø¨Ù„ Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ø®Ø·ÛŒ Ù‡Ø³ØªÙ†Ø¯ØŒ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª. Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ú†ÙˆÙ† Iris Ù‚Ø§Ø¨Ù„ Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ø®Ø·ÛŒ Ø§Ø³ØªØŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª.\n",
    "\n",
    "Ø¨Ù‡ Ø·ÙˆØ± Ú©Ù„ÛŒØŒ Perceptron ÛŒÚ© Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø³Ø§Ø¯Ù‡ Ùˆ Ø³Ø±ÛŒØ¹ Ø§Ø³ØªØŒ Ø§Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ±ØŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ±ÛŒ Ù…Ø«Ù„ SVM ÛŒØ§ Neural Networks Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ù‡Ø³ØªÙ†Ø¯.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
